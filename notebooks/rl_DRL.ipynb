{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee80a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:45:49.491535Z",
     "iopub.status.busy": "2024-08-14T10:45:49.491336Z",
     "iopub.status.idle": "2024-08-14T10:45:49.497833Z",
     "shell.execute_reply": "2024-08-14T10:45:49.497081Z"
    },
    "papermill": {
     "duration": 0.011972,
     "end_time": "2024-08-14T10:45:49.500122",
     "exception": false,
     "start_time": "2024-08-14T10:45:49.488150",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#initialise for papermill\n",
    "iteration = 4\n",
    "error_type = \"gauss\"\n",
    "num_iterations = 100_000\n",
    "seed = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fa121a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:45:49.506508Z",
     "iopub.status.busy": "2024-08-14T10:45:49.506153Z",
     "iopub.status.idle": "2024-08-14T10:45:49.509383Z",
     "shell.execute_reply": "2024-08-14T10:45:49.508719Z"
    },
    "papermill": {
     "duration": 0.0084,
     "end_time": "2024-08-14T10:45:49.510810",
     "exception": false,
     "start_time": "2024-08-14T10:45:49.502410",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "error_type = \"both\"\n",
    "iteration = 7\n",
    "num_iterations = 50000\n",
    "seed = 735\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094633de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:45:49.514955Z",
     "iopub.status.busy": "2024-08-14T10:45:49.514593Z",
     "iopub.status.idle": "2024-08-14T10:45:49.517642Z",
     "shell.execute_reply": "2024-08-14T10:45:49.516977Z"
    },
    "papermill": {
     "duration": 0.006783,
     "end_time": "2024-08-14T10:45:49.519062",
     "exception": false,
     "start_time": "2024-08-14T10:45:49.512279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 100  \n",
    "min_delta = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c6d040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:45:49.522705Z",
     "iopub.status.busy": "2024-08-14T10:45:49.522464Z",
     "iopub.status.idle": "2024-08-14T10:45:54.594558Z",
     "shell.execute_reply": "2024-08-14T10:45:54.594264Z"
    },
    "papermill": {
     "duration": 5.075509,
     "end_time": "2024-08-14T10:45:54.595682",
     "exception": false,
     "start_time": "2024-08-14T10:45:49.520173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import shutil\n",
    "import zipfile\n",
    "# from google.colab import files\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gym\n",
    "import quanttrader as qt\n",
    "import pyfolio as pf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential, q_network, network\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "# iteration = 4\n",
    "eval_interval = 1_00\n",
    "log_interval = 1_00\n",
    "\n",
    "gym.__version__, qt.__version__, pf.__version__\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Gym trading env\n",
    "Unlike live engine or backtest engine, where event loops are driven by live ticks or historical ticks,\n",
    "here it is driven by step function, similar to\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "The sequence is\n",
    "1. obs <- env.reset()      # env; obs = OHLCV + technical indicators + holdings\n",
    "2. action <- pi(obs)       # agent; action = target weights\n",
    "3. news_obs, reward, <- step(action)      # env\n",
    "    3.a action to orders   # sum(holdings) * new weights\n",
    "    3.b fill orders        # portfolio rotates into new holdings\n",
    "repeat 2, and 3 to interact between agent and env\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        backtest gym engine\n",
    "        it doesn't normalize; and expects a normalization layer\n",
    "    Observation:\n",
    "        Type: Box(lookback_window, n_assets*5+2)\n",
    "        lookback_window x (n_assets*(ohlcv) + cash+npv)\n",
    "        TODO: append trades, standing orders, etc\n",
    "        TODO: stop/limit orders\n",
    "    Actions:\n",
    "        Type: Box(n_assets + 1)\n",
    "        portfolio weights [w1,w2...w_k, cash_weight], add up to one\n",
    "    Reward:\n",
    "        pnl every day, similar to space-invaders\n",
    "    Starting State:\n",
    "        random timestamp between start_date and (end_date - run_window)\n",
    "    Episode Termination:\n",
    "        after predefined window\n",
    "        If broke, no orders will send\n",
    "    \"\"\"\n",
    "    def __init__(self, n: np.int32, df_obs_scaled : pd.DataFrame, df_exch : pd.DataFrame):\n",
    "        assert n >= 2\n",
    "\n",
    "        self._df_obs_scaled = df_obs_scaled     # observation; scaled outside along with TA indicators\n",
    "        self._df_exch = df_exch                 # exch\n",
    "        self._df_positions = df_exch * 0.0      # positions plus cash plus nav\n",
    "        self._df_positions['Cash'] = 0.0\n",
    "        self._df_positions['NAV'] = 0.0\n",
    "\n",
    "        self._asset_syms = df_exch.columns\n",
    "        self._n_assets = len(self._asset_syms)\n",
    "        self._n_features = df_obs_scaled.shape[1] / self._n_assets      # assume same features across assets\n",
    "\n",
    "        self._inital_cash = 100_000.0\n",
    "        self._cash = self._inital_cash\n",
    "        self._commission_rate = 0.0\n",
    "        self._look_back = 10                        # observation lookback history\n",
    "        self._warmup = 50                           # observation ramp-up due to e.g. 10 periods are required to cacl MA(10)\n",
    "        self._maxsteps = 252                        # max steps in one episode\n",
    "\n",
    "        self._max_nav_scaler = 1.0\n",
    "        self._lock_init_step = False\n",
    "        self._init_step = 0\n",
    "        self._current_step = 0\n",
    "\n",
    "        self._n = n             # n=2: buy/sell 100% or [0, 100%]; n=3: [0, 50%, 100%]; n=4: [0, 33.3%, 66.7%, 100%]\n",
    "        self._pcts = [pct / (self._n-1) for pct in range(self._n)]\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(n=self._n)\n",
    "        # first col is open, second col is high, ..., last col is nav\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self._look_back, df_obs_scaled.shape[1]+1), dtype=np.float32)\n",
    "                    \n",
    "    def set_cash(self, cash: np.float32=100_000.0):\n",
    "        self._inital_cash = cash\n",
    "        self._cash = self._inital_cash\n",
    "\n",
    "    def set_commission(self, comm: np.float32=0.0001):\n",
    "        \"\"\"\n",
    "        commission plus slippage\n",
    "        \"\"\"\n",
    "        self._commission_rate = comm\n",
    "\n",
    "    def set_steps(self, n_lookback : np.int32=10, n_warmup : np.int32=50, n_maxsteps: np.int32=252, n_init_step: np.int32=0):\n",
    "        self._lookback = n_lookback\n",
    "        self._warmup = n_warmup\n",
    "        self._maxsteps = n_maxsteps\n",
    "        self._init_step = n_init_step\n",
    "        if n_init_step > 0:\n",
    "            assert n_init_step >= n_warmup\n",
    "            self._lock_init_step = True\n",
    "\n",
    "    def set_feature_scaling(self, max_nav_scaler : np.float32=1.0):\n",
    "        self._max_nav_scaler = max_nav_scaler\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        return an array of size self._lookback x features.\n",
    "        Each column is a feature; last feature is NAV.\n",
    "        Row is in time ascending order. That is, last row is self._current_step.\n",
    "        \"\"\"\n",
    "        obs = self._df_obs_scaled.iloc[self._current_step-self._look_back+1:self._current_step+1].values\n",
    "        obs = np.append(obs, self._df_positions.iloc[self._current_step-self._look_back+1:self._current_step+1][['NAV']].values / self._max_nav_scaler, axis=1)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        move one step to the next timestamp, accordingly to action\n",
    "        assume hft condition: execution at today 15:59:59, after observing today's ohl and (almost) close.\n",
    "        execution immediately using market or market on close, no slippage.\n",
    "        e.g., assume on 12/31/2019, 1/2/2020, and 1/3/2020 prices are $95, $100, $110. respectively. \n",
    "        The state or observation is prices of last two days. \n",
    "        We start on 1/2/2020 with $100,000.\n",
    "        Then\n",
    "        1. on 1/2/2020, obs = [$95, $100]         # obs <- env.reset()\n",
    "        2. on 1/2/2020, based on the up-trend observation, we decide to buy. \n",
    "            Our allocation action w is [50%, 50%], or half in cash, half in stock.\n",
    "        3. on 1/2/2020, the step function is      # news_obs, reward <- step(action)\n",
    "            3.a buy order of $5,000 or 50 shares, filled at $100\n",
    "            3.b stock market environment transits to 1/3/2020, new observation is [$100, $110];\n",
    "                our stock is now worth $5,500, and total asset NAV is $10,500, or reward $500.\n",
    "        :param action:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        done = False\n",
    "\n",
    "        current_size = int(self._df_positions.iloc[self._current_step][self._asset_syms].item())\n",
    "        current_cash = self._df_positions.iloc[self._current_step]['Cash']\n",
    "        current_price = self._df_exch.iloc[self._current_step].item()\n",
    "\n",
    "        # rebalance\n",
    "        current_nav = current_cash + current_price * current_size       # should equal to the nav column\n",
    "        new_size = int(np.floor(current_nav * self._pcts[action] / current_price))       # odd size allowed; action[-1] is cash\n",
    "        delta_size = new_size - current_size\n",
    "        current_commission = np.abs(delta_size) * current_price * self._commission_rate\n",
    "        new_cash = current_cash - delta_size * current_price - current_commission\n",
    "\n",
    "        # move to next timestep\n",
    "        self._current_step += 1\n",
    "        new_price = self._df_exch.iloc[self._current_step].item()\n",
    "        new_nav = new_cash + new_price * new_size\n",
    "        reward = (new_price - current_price) * new_size - current_commission     # commission is penalty\n",
    "        info = {'step': self._current_step, 'time': self._df_obs_scaled.index[self._current_step],\n",
    "                'old_price': current_price, 'old position': current_size, 'old_cash': current_cash, 'old_nav': current_nav,\n",
    "                'price': new_price, 'position': new_size, 'cash': new_cash, 'nav': new_nav,\n",
    "                'transaction': delta_size, 'commission': current_commission, 'nav_diff': new_nav-current_nav}     # reward = new_nav - current_nav\n",
    "\n",
    "        # reward = reward / self._max_nav_scaler\n",
    "        self._df_positions.loc[self._df_positions.index[self._current_step], self._asset_syms] = new_size\n",
    "        self._df_positions['Cash'][self._current_step] = new_cash\n",
    "        self._df_positions['NAV'][self._current_step] = new_nav\n",
    "\n",
    "        if self._current_step - self._init_step >= self._maxsteps:      # e.g. init=3, current=7, _maxsteps=4\n",
    "            done = True\n",
    "        if self._current_step == self._df_exch.shape[0]-1:              # end of data\n",
    "            done = True\n",
    "\n",
    "        # s'\n",
    "        new_state = self._get_observation()\n",
    "\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        random start time\n",
    "        \"\"\"\n",
    "        self._cash = self._inital_cash\n",
    "        self._df_positions = self._df_exch * 0.0\n",
    "        self._df_positions['Cash'] = 0.0\n",
    "        self._df_positions['NAV'] = 0.0\n",
    "        \n",
    "        if not self._lock_init_step:\n",
    "            self._init_step = np.random.randint(low=self._warmup-1, high=self._df_obs_scaled.shape[0]-self._maxsteps)    # low (inclusive) to high (exclusive)\n",
    "        self._current_step = self._init_step\n",
    "\n",
    "        self._df_positions['Cash'][:self._current_step+1] = self._cash\n",
    "        self._df_positions['NAV'][:self._current_step+1] = self._cash\n",
    "\n",
    "        # return current_step\n",
    "        return self._get_observation()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        plt.rcParams.update({'font.size': 10})  # Adjust font size globally\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(15, 8), gridspec_kw={'height_ratios': [3, 1]})  # Increase figure size\n",
    "        fig.suptitle(f'PPO Agent: {error_type.capitalize()} Error, Dataset {iteration}, {dataset_type}', fontsize=12)        # Adjust layout\n",
    "        plt.subplots_adjust(\n",
    "            left=0.1,  # left side margin\n",
    "            right=0.9,  # right side margin\n",
    "            bottom=0.1,  # bottom margin\n",
    "            top=0.9,  # top margin\n",
    "            hspace=0.4,  # height space between subplots\n",
    "        )\n",
    "        # fig.tight_layout()\n",
    "        x_left = self._init_step\n",
    "        x_right = self._current_step+1\n",
    "        x_end = min(self._df_exch.shape[0] - 1, self._init_step + self._maxsteps)\n",
    "        # x_end = min(self._df_exch.shape[0], self._init_step+self._maxsteps+1)\n",
    "        df_price = self._df_exch[x_left:x_right]\n",
    "        df_nav = self._df_positions['NAV'][x_left:x_right]\n",
    "\n",
    "        # Market Price and Trading Signals\n",
    "        ax[0].set_title('Market Price and Trading Signals')\n",
    "        ax[0].set_xlabel('Time')  # Set x-axis label\n",
    "        ax[0].set_ylabel('Price')  # Set y-axis label\n",
    "        ax[0].tick_params(\n",
    "            axis='x',          \n",
    "            which='both',      \n",
    "            bottom=True,  # Enable ticks along the bottom edge\n",
    "            top=False,         \n",
    "            labelbottom=True)  # Enable labels along the bottom edge\n",
    "        ax[0].set_xlim([self._df_exch.index[x_left], self._df_exch.index[x_end]])\n",
    "        ax[0].set_ylim([max(self._df_exch[x_left:x_end].min().item()-5, 0), self._df_exch[x_left:x_end].max().item()+5])\n",
    "        df_position = self._df_positions['NAV'][x_left:x_right]\n",
    "        df_position_diff = df_position - df_position.shift(1)\n",
    "        df_buy = df_price[df_position_diff > 0]\n",
    "        df_sell = df_price[df_position_diff < 0]\n",
    "        ax[0].plot(df_price, color='black', label='Price')\n",
    "        ax[0].plot(df_buy, '^', markersize=5, color='r')\n",
    "        ax[0].plot(df_sell, 'v', markersize=5, color='g')\n",
    "\n",
    "        # Net Asset Value (NAV) Over Time\n",
    "        ax[1].set_title('Net Asset Value (NAV) Over Time')\n",
    "        ax[1].set_xlabel('Time')  # Set x-axis label\n",
    "        ax[1].set_ylabel('NAV')  # Set y-axis label\n",
    "        ax[1].set_xlim([self._df_exch.index[x_left], self._df_exch.index[x_end]])\n",
    "        ax[1].plot(df_nav, color='black', label='NAV')\n",
    "\n",
    "        #plt.pause(0.001)\n",
    "\n",
    "        # https://stackoverflow.com/questions/7821518/matplotlib-save-plot-to-numpy-array\n",
    "        fig.canvas.draw()\n",
    "        data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close()\n",
    "        return data\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Render function for trades\n",
    "\n",
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)\n",
    "\n",
    "def create_policy_eval_video(env, policy, filename, num_episodes=5, fps=30):\n",
    "  filename = \"results/videos/\" +filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      video.append_data(env.pyenv.envs[0].render())\n",
    "\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = env.step(action_step.action)\n",
    "        video.append_data(env.pyenv.envs[0].render())\n",
    "\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "def load_data():\n",
    "    from datetime import timedelta\n",
    "    import ta\n",
    "\n",
    "    start_date = datetime(2010, 1, 1)\n",
    "    end_date = datetime(2020, 12, 31)\n",
    "    syms = ['SPY']\n",
    "    max_price_scaler = 5_000.0\n",
    "    max_price_scaler = 1\n",
    "    max_volume_scaler = 1.5e8\n",
    "    df_obs = pd.DataFrame()             # observation\n",
    "    df_exch = pd.DataFrame()            # exchange; for order match\n",
    "\n",
    "    for sym in syms:\n",
    "        df = yf.download(sym, start=start_date, end=end_date)\n",
    "        df.index = pd.to_datetime(df.index) + timedelta(hours=15, minutes=59, seconds=59)\n",
    "\n",
    "        df_exch = pd.concat([df_exch, df['Close'].rename(sym)], axis=1)\n",
    "\n",
    "        df['Open'] = df['Adj Close'] / df['Close'] * df['Open'] / max_price_scaler\n",
    "        df['High'] = df['Adj Close'] / df['Close'] * df['High'] / max_price_scaler\n",
    "        df['Low'] = df['Adj Close'] / df['Close'] * df['Low'] / max_price_scaler\n",
    "        df['Volume'] = df['Adj Close'] / df['Close'] * df['Volume'] / max_volume_scaler\n",
    "        df['Close'] = df['Adj Close'] / max_price_scaler\n",
    "        df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "        df.columns = [f'{sym}:{c.lower()}' for c in df.columns]\n",
    "\n",
    "        macd = ta.trend.MACD(close=df[f'{sym}:close'])\n",
    "        df[f'{sym}:macd'] = macd.macd()\n",
    "        df[f'{sym}:macd_diff'] = macd.macd_diff()\n",
    "        df[f'{sym}:macd_signal'] = macd.macd_signal()\n",
    "\n",
    "        rsi = ta.momentum.RSIIndicator(close=df[f'{sym}:close'])\n",
    "        df[f'{sym}:rsi'] = rsi.rsi()\n",
    "\n",
    "        bb = ta.volatility.BollingerBands(close=df[f'{sym}:close'], window=20, window_dev=2)\n",
    "        df[f'{sym}:bb_bbm'] = bb.bollinger_mavg()\n",
    "        df[f'{sym}:bb_bbh'] = bb.bollinger_hband()\n",
    "        df[f'{sym}:bb_bbl'] = bb.bollinger_lband()\n",
    "\n",
    "        atr = ta.volatility.AverageTrueRange(high=df[f'{sym}:high'], low=df[f'{sym}:low'], close=df[f'{sym}:close'])\n",
    "        df[f'{sym}:atr'] = atr.average_true_range()\n",
    "\n",
    "        df_obs = pd.concat([df_obs, df], axis=1)\n",
    "\n",
    "    return df_obs, df_exch\n",
    "\n",
    "def setindexdate(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date', drop=True)\n",
    "    df = df.drop(columns = 'tic')\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=5):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "    zeros = 0\n",
    "    ones = 0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      if action_step.action.numpy()[0] == 1:\n",
    "        ones+=1\n",
    "      else:\n",
    "        zeros+=1\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0], zeros, ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbde34f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:45:54.599720Z",
     "iopub.status.busy": "2024-08-14T10:45:54.599441Z",
     "iopub.status.idle": "2024-08-14T11:11:44.049387Z",
     "shell.execute_reply": "2024-08-14T11:11:44.048906Z"
    },
    "papermill": {
     "duration": 1549.476207,
     "end_time": "2024-08-14T11:11:44.073425",
     "exception": false,
     "start_time": "2024-08-14T10:45:54.597218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  22100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32503 (126.96 KB)\n",
      "Trainable params: 32503 (126.96 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:364: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered\n",
      "Metrics saved to training_metrics_7_both.csv\n"
     ]
    }
   ],
   "source": [
    "train = setindexdate(pd.read_parquet(f\"datasets/rl_train_dataset{iteration}.parquet\")).astype('float64')\n",
    "trade = setindexdate(pd.concat([pd.read_parquet(f\"datasets/rl_val_dataset{iteration}.parquet\"),pd.read_parquet(f\"datasets/rl_test_dataset{iteration}.parquet\")])).astype('float64')\n",
    "\n",
    "match(error_type):\n",
    "    case \"gauss\": \n",
    "        train_df_obs = train.drop(columns=['errors', 'TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        train_df_exch = train_df_obs[['open']]\n",
    "        trade_df_obs = trade.drop(columns=['errors', 'TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        trade_df_exch = trade_df_obs[['open']]\n",
    "    case \"ae\": \n",
    "        train_df_obs = train.drop(columns=['gauss_score', 'TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        train_df_exch = train_df_obs[['open']]\n",
    "        trade_df_obs = trade.drop(columns=['gauss_score', 'TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        trade_df_exch = trade_df_obs[['open']]\n",
    "    case \"both\": \n",
    "        train_df_obs = train.drop(columns=['TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        train_df_exch = train_df_obs[['open']]\n",
    "        trade_df_obs = trade.drop(columns=['TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        trade_df_exch = trade_df_obs[['open']]\n",
    "    case \"na\":\n",
    "        train_df_obs = train.drop(columns=['gauss_score', 'errors', 'TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        train_df_exch = train_df_obs[['open']]\n",
    "        trade_df_obs = trade.drop(columns=['gauss_score', 'errors', 'TEDRATE error', 'AAAFF error', 'DTB3 error',\n",
    "       'S&P 500 COMPOSITE - DS DIVIDEND YIELD error', 'SnP PRICE error'])\n",
    "        trade_df_exch = trade_df_obs[['open']]\n",
    "\n",
    "\n",
    "df_obs = pd.concat([train_df_obs, trade_df_obs])\n",
    "df_exch = pd.concat([train_df_exch, trade_df_exch])\n",
    "look_back = 5\n",
    "cash = 100_000.0\n",
    "max_nav_scaler = cash\n",
    "\n",
    "train_qt_env = TradingEnv(3, train_df_obs, train_df_exch)\n",
    "train_qt_env.set_cash(cash)\n",
    "train_qt_env.set_commission(0.0001)\n",
    "train_qt_env.set_steps(n_lookback=10, n_warmup=50, n_maxsteps=250)\n",
    "train_qt_env.set_feature_scaling(max_nav_scaler)\n",
    "\n",
    "eval_qt_env = TradingEnv(3, df_obs, df_exch)\n",
    "eval_qt_env.set_cash(cash)\n",
    "eval_qt_env.set_commission(0.0001)\n",
    "eval_qt_env.set_steps(n_lookback=10, n_warmup=50, n_maxsteps=2000, n_init_step=len(train_df_obs)) \n",
    "eval_qt_env.set_feature_scaling(max_nav_scaler)\n",
    "\n",
    "\n",
    "train_qt_env = gym.wrappers.FlattenObservation(train_qt_env)\n",
    "train_py_env = suite_gym.wrap_env(train_qt_env)\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "\n",
    "eval_qt_env = gym.wrappers.FlattenObservation(eval_qt_env)\n",
    "eval_py_env = suite_gym.wrap_env(eval_qt_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "num_eval_episodes = 10\n",
    "replay_buffer_max_length = 100000\n",
    "\n",
    "fc_layer_params = (100, 100)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    epsilon_greedy=0.01,\n",
    "    gamma = 0.99,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "print(q_net.summary())\n",
    "\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=100_000)\n",
    "\n",
    "train_env.reset()\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2).prefetch(3)\n",
    "iterator = iter(dataset)\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = DynamicStepDriver(\n",
    "    train_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=5) # collect 4 steps for each training iteration\n",
    "\n",
    "agent.train_step_counter.assign(0)\n",
    "time_step = None\n",
    "policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
    "\n",
    "steps_since_last_improvement = 0\n",
    "best_metric = -float('inf')\n",
    "\n",
    "returns = np.array([])\n",
    "metrics = []\n",
    "\n",
    "while True:\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "    if step % log_interval == 0:\n",
    "        avg_return = compute_avg_return(train_env, agent.policy, num_episodes=1)[0]\n",
    "        returns = np.append(returns, avg_return)\n",
    "        \n",
    "        # Store metrics in the list\n",
    "        metrics.append({'Step': step, 'Average Return': avg_return, 'Train Loss': train_loss.numpy()})\n",
    "\n",
    "        # Check if there is an improvement\n",
    "        if avg_return >= (best_metric + min_delta):\n",
    "            best_metric = avg_return\n",
    "            steps_since_last_improvement = 0\n",
    "        else:\n",
    "            steps_since_last_improvement += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if steps_since_last_improvement >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    if step > num_iterations:\n",
    "        break\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = f'training_metrics_{iteration}_{error_type}.csv'\n",
    "\n",
    "# Convert the metrics list to a DataFrame\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_metrics.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Metrics saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c56e199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T11:11:44.083987Z",
     "iopub.status.busy": "2024-08-14T11:11:44.083780Z",
     "iopub.status.idle": "2024-08-14T11:12:08.116980Z",
     "shell.execute_reply": "2024-08-14T11:12:08.116457Z"
    },
    "papermill": {
     "duration": 24.03971,
     "end_time": "2024-08-14T11:12:08.118601",
     "exception": false,
     "start_time": "2024-08-14T11:11:44.078891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1500, 800) to (1504, 800) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1500, 800) to (1504, 800) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1824.2832, 196, 54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2365.6384, 60, 12)\n"
     ]
    }
   ],
   "source": [
    "# tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
    "# tf_policy_saver.save(f\"/Users/justin/Desktop/URECA/saved_models/DRL_{error_type}_trained_agent_\"+str(iteration))\n",
    "\n",
    "dataset_type = 'Train'\n",
    "create_policy_eval_video(train_env, agent.policy, f\"DRL_{error_type}_trained_agent_train_\"+str(iteration), num_episodes=1)\n",
    "print(compute_avg_return(train_env, agent.policy, num_episodes=1))\n",
    "\n",
    "x_left = train_env.pyenv.envs[0].env._init_step\n",
    "x_right = train_env.pyenv.envs[0].env._current_step     # _maxsteps+1\n",
    "df_price = train_env.pyenv.envs[0].env._df_exch[x_left:x_right].copy()\n",
    "df_reinforcement = train_env.pyenv.envs[0].env._df_positions['NAV'][x_left:x_right].copy()\n",
    "df_orders = train_env.pyenv.envs[0].env._df_positions.copy()\n",
    "df_orders['action'] = df_orders['open'].diff().fillna(0)\n",
    "df_all = pd.concat([df_reinforcement, df_price], axis=1)\n",
    "df_all.columns = ['tf-agent', 'benchmark']\n",
    "df_ret = df_all / df_all.shift(1) - 1\n",
    "df_ret = df_ret[1:]\n",
    "\n",
    "agent_perf_stats = pf.timeseries.perf_stats(df_ret['tf-agent'])\n",
    "benchmark_perf_stats = pf.timeseries.perf_stats(df_ret['benchmark'])\n",
    "perf_stats = pd.concat([agent_perf_stats, benchmark_perf_stats], axis=1)\n",
    "perf_stats.columns = ['tf-agent', 'benchmark']\n",
    "perf_stats\n",
    "\n",
    "df_all.to_parquet(path=f\"results/DRL_{error_type}_returns_train_{iteration}.parquet\")\n",
    "df_orders.to_parquet(path=f\"results/DRL_{error_type}_orders_train_{iteration}.parquet\")\n",
    "\n",
    "dataset_type = 'Test'\n",
    "create_policy_eval_video(eval_env, agent.policy, f\"DRL_{error_type}_trained_agent_test_\"+str(iteration), num_episodes=1)\n",
    "print(compute_avg_return(eval_env, agent.policy, num_episodes=1))\n",
    "\n",
    "x_left = eval_env.pyenv.envs[0].env._init_step\n",
    "x_right = eval_env.pyenv.envs[0].env._current_step     # _maxsteps+1\n",
    "df_price = eval_env.pyenv.envs[0].env._df_exch[x_left:x_right].copy()\n",
    "df_reinforcement = eval_env.pyenv.envs[0].env._df_positions['NAV'][x_left:x_right].copy()\n",
    "df_orders = eval_env.pyenv.envs[0].env._df_positions.copy()\n",
    "df_orders['action'] = df_orders['open'].diff().fillna(0)\n",
    "df_all = pd.concat([df_reinforcement, df_price], axis=1)\n",
    "df_all.columns = ['tf-agent', 'benchmark']\n",
    "df_ret = df_all / df_all.shift(1) - 1\n",
    "df_ret = df_ret[1:]\n",
    "\n",
    "agent_perf_stats = pf.timeseries.perf_stats(df_ret['tf-agent'])\n",
    "benchmark_perf_stats = pf.timeseries.perf_stats(df_ret['benchmark'])\n",
    "perf_stats = pd.concat([agent_perf_stats, benchmark_perf_stats], axis=1)\n",
    "perf_stats.columns = ['tf-agent', 'benchmark']\n",
    "perf_stats\n",
    "\n",
    "df_all.to_parquet(path=f\"results/DRL_{error_type}_returns_test_{iteration}.parquet\")\n",
    "df_orders.to_parquet(path=f\"results/DRL_{error_type}_orders_test_{iteration}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ureca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1583.048555,
   "end_time": "2024-08-14T11:12:11.468292",
   "environment_variables": {},
   "exception": null,
   "input_path": "rl_DRL.ipynb",
   "output_path": "rl_DRL.ipynb",
   "parameters": {
    "error_type": "both",
    "iteration": 7,
    "num_iterations": 50000,
    "seed": 735
   },
   "start_time": "2024-08-14T10:45:48.419737",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}