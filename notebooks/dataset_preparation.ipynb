{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cee1d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:58:58.937145Z",
     "iopub.status.busy": "2024-05-12T16:58:58.936734Z",
     "iopub.status.idle": "2024-05-12T16:58:58.944408Z",
     "shell.execute_reply": "2024-05-12T16:58:58.943945Z"
    },
    "papermill": {
     "duration": 0.015688,
     "end_time": "2024-05-12T16:58:58.945798",
     "exception": false,
     "start_time": "2024-05-12T16:58:58.930110",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e090945b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:58:58.954068Z",
     "iopub.status.busy": "2024-05-12T16:58:58.953841Z",
     "iopub.status.idle": "2024-05-12T16:58:58.956244Z",
     "shell.execute_reply": "2024-05-12T16:58:58.955899Z"
    },
    "papermill": {
     "duration": 0.007828,
     "end_time": "2024-05-12T16:58:58.957371",
     "exception": false,
     "start_time": "2024-05-12T16:58:58.949543",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "iteration = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8a69588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:58:58.962220Z",
     "iopub.status.busy": "2024-05-12T16:58:58.962064Z",
     "iopub.status.idle": "2024-05-12T16:58:58.964476Z",
     "shell.execute_reply": "2024-05-12T16:58:58.964110Z"
    },
    "papermill": {
     "duration": 0.006368,
     "end_time": "2024-05-12T16:58:58.965931",
     "exception": false,
     "start_time": "2024-05-12T16:58:58.959563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_ = {\n",
    "    'input_dim': (5,5),\n",
    "    'encoding_dim': 100,\n",
    "    'input_size': (4,5),\n",
    "    'num_layers': 1,\n",
    "    'output_size': 1,\n",
    "    'n_future': 1,\n",
    "    'n_past': 5   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "869ad468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:58:58.970663Z",
     "iopub.status.busy": "2024-05-12T16:58:58.970506Z",
     "iopub.status.idle": "2024-05-12T16:59:02.110494Z",
     "shell.execute_reply": "2024-05-12T16:59:02.110204Z"
    },
    "papermill": {
     "duration": 3.143502,
     "end_time": "2024-05-12T16:59:02.111451",
     "exception": false,
     "start_time": "2024-05-12T16:58:58.967949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_uncertainty\n",
    "from keras_uncertainty.layers import StochasticDropout\n",
    "from keras_uncertainty.models import StochasticRegressor\n",
    "from keras_uncertainty.utils import regressor_calibration_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63fe4cf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:59:02.115881Z",
     "iopub.status.busy": "2024-05-12T16:59:02.115613Z",
     "iopub.status.idle": "2024-05-12T16:59:02.430027Z",
     "shell.execute_reply": "2024-05-12T16:59:02.429638Z"
    },
    "papermill": {
     "duration": 0.317697,
     "end_time": "2024-05-12T16:59:02.431162",
     "exception": false,
     "start_time": "2024-05-12T16:59:02.113465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/7k75vhv540d_lhcr6xp_xrzw0000gn/T/ipykernel_69716/4287731575.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  AAAFF = pd.read_csv('raw_data/AAAFF.csv').fillna(method = 'ffill').dropna()\n",
      "/var/folders/5_/7k75vhv540d_lhcr6xp_xrzw0000gn/T/ipykernel_69716/4287731575.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  DTB3 = pd.read_csv('raw_data/DTB3.csv').fillna(method = 'ffill').dropna()\n",
      "/var/folders/5_/7k75vhv540d_lhcr6xp_xrzw0000gn/T/ipykernel_69716/4287731575.py:3: FutureWarning: DatetimeIndexResampler.fillna is deprecated and will be removed in a future version. Use obj.ffill(), obj.bfill(), or obj.nearest() instead.\n",
      "  SnP_div = pd.read_excel('raw_data/S&P Dividend Yield 1963-2023.xlsx').rename(columns={'Name':'DATE'}).set_index('DATE').resample('d').fillna(method = 'ffill').dropna().reset_index()\n",
      "/var/folders/5_/7k75vhv540d_lhcr6xp_xrzw0000gn/T/ipykernel_69716/4287731575.py:5: FutureWarning: DatetimeIndexResampler.fillna is deprecated and will be removed in a future version. Use obj.ffill(), obj.bfill(), or obj.nearest() instead.\n",
      "  SnP = SnP.rename(columns={SnP.columns[0]:'DATE',SnP.columns[1]:'SnP PRICE'}).set_index('DATE').resample('d').fillna(method = 'ffill').dropna().reset_index()\n",
      "/var/folders/5_/7k75vhv540d_lhcr6xp_xrzw0000gn/T/ipykernel_69716/4287731575.py:6: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  TED = pd.read_csv('raw_data/TEDRATE.csv').fillna(method = 'ffill').dropna()\n",
      "/var/folders/5_/7k75vhv540d_lhcr6xp_xrzw0000gn/T/ipykernel_69716/4287731575.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.replace('.', None).fillna(method = 'ffill').dropna()\n"
     ]
    }
   ],
   "source": [
    "AAAFF = pd.read_csv('raw_data/AAAFF.csv').fillna(method = 'ffill').dropna()\n",
    "DTB3 = pd.read_csv('raw_data/DTB3.csv').fillna(method = 'ffill').dropna()\n",
    "SnP_div = pd.read_excel('raw_data/S&P Dividend Yield 1963-2023.xlsx').rename(columns={'Name':'DATE'}).set_index('DATE').resample('d').fillna(method = 'ffill').dropna().reset_index()\n",
    "SnP = pd.read_excel('raw_data/SP500Index_Retrieved_26_06_2022.xlsx').loc[5:]\n",
    "SnP = SnP.rename(columns={SnP.columns[0]:'DATE',SnP.columns[1]:'SnP PRICE'}).set_index('DATE').resample('d').fillna(method = 'ffill').dropna().reset_index()\n",
    "TED = pd.read_csv('raw_data/TEDRATE.csv').fillna(method = 'ffill').dropna()\n",
    "for i, item in enumerate([TED, AAAFF, DTB3, SnP_div, SnP]):\n",
    "    item['DATE'] = pd.to_datetime(item['DATE'])\n",
    "    if i == 0:\n",
    "        data = item.copy().set_index('DATE')\n",
    "    else:\n",
    "        data = data.join(item.set_index('DATE'), on='DATE', how='left')\n",
    "data = data.replace('.', None).fillna(method = 'ffill').dropna()\n",
    "data = data.astype(float)\n",
    "\n",
    "#MinMax Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_data = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n",
    "unscaler = MinMaxScaler()\n",
    "unscaler = unscaler.fit(data[['SnP PRICE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60de4e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:59:02.435345Z",
     "iopub.status.busy": "2024-05-12T16:59:02.435203Z",
     "iopub.status.idle": "2024-05-12T16:59:02.442321Z",
     "shell.execute_reply": "2024-05-12T16:59:02.442057Z"
    },
    "papermill": {
     "duration": 0.010106,
     "end_time": "2024-05-12T16:59:02.443241",
     "exception": false,
     "start_time": "2024-05-12T16:59:02.433135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 4: 925\n",
      "Data 5: 2043\n",
      "Data 6: 1172\n",
      "Data 7: 2216\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = scaled_data.loc[\"1971-02-05\":\"1973-03-07\"]\n",
    "val_data_1 = scaled_data.loc[\"1973-06-12\":\"1974-03-26\"]\n",
    "test_data_1 = scaled_data.loc[\"1974-03-28\":\"1976-02-02\"]\n",
    "\n",
    "train_data_2 = scaled_data.loc[\"1975-08-19\":\"1979-05-18\"]\n",
    "val_data_2 = scaled_data.loc[\"1979-11-06\":\"1980-02-25\"]\n",
    "test_data_2 = scaled_data.loc[\"1980-02-26\":\"1980-11-06\"]\n",
    "\n",
    "train_data_3 = scaled_data.loc[\"1980-07-24\":\"1981-01-13\"]\n",
    "val_data_3 = scaled_data.loc[\"1981-02-04\":\"1981-11-23\"]\n",
    "test_data_3 = scaled_data.loc[\"1981-11-24\":\"1983-10-07\"]\n",
    "\n",
    "train_data_4 = scaled_data.loc[\"1983-08-02\":\"1989-07-19\"]\n",
    "val_data_4 = scaled_data.loc[\"1990-04-19\":\"1990-09-12\"]\n",
    "test_data_4 = scaled_data.loc[\"1990-09-13\":\"1991-08-19\"]\n",
    "dataset_4 = {'train': train_data_4, 'val': val_data_4, 'test': test_data_4}\n",
    "\n",
    "train_data_5 = scaled_data.loc[\"1992-02-24\":\"1999-12-22\"]\n",
    "val_data_5 = scaled_data.loc[\"2000-12-15\":\"2001-05-10\"]\n",
    "test_data_5 = scaled_data.loc[\"2001-05-11\":\"2002-04-24\"]\n",
    "dataset_5 = {'train': train_data_5, 'val': val_data_5, 'test': test_data_5}\n",
    "\n",
    "train_data_6 = scaled_data.loc[\"2002-05-30\":\"2006-11-24\"]\n",
    "val_data_6 = scaled_data.loc[\"2007-06-22\":\"2008-05-14\"]\n",
    "test_data_6 = scaled_data.loc[\"2008-05-15\":\"2010-06-16\"]\n",
    "dataset_6 = {'train': train_data_6, 'val': val_data_6, 'test': test_data_6}\n",
    "\n",
    "train_data_7 = scaled_data.loc[\"2010-06-23\":\"2018-12-19\"]\n",
    "val_data_7 = scaled_data.loc[\"2020-01-14\":\"2020-02-20\"]\n",
    "test_data_7 = scaled_data.loc[\"2020-02-21\":\"2020-05-13\"]\n",
    "dataset_7 = {'train': train_data_7, 'val': val_data_7, 'test': test_data_7}\n",
    "\n",
    "# print(f\"Data 1: {len(train_data_1)}\")\n",
    "# print(f\"Data 2: {len(train_data_2)}\")\n",
    "# print(f\"Data 3: {len(train_data_3)}\")\n",
    "print(f\"Data 4: {len(train_data_4)}\")\n",
    "print(f\"Data 5: {len(train_data_5)}\")\n",
    "print(f\"Data 6: {len(train_data_6)}\")\n",
    "print(f\"Data 7: {len(train_data_7)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29dcd5f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:59:02.446979Z",
     "iopub.status.busy": "2024-05-12T16:59:02.446761Z",
     "iopub.status.idle": "2024-05-12T16:59:02.449088Z",
     "shell.execute_reply": "2024-05-12T16:59:02.448690Z"
    },
    "papermill": {
     "duration": 0.005222,
     "end_time": "2024-05-12T16:59:02.450092",
     "exception": false,
     "start_time": "2024-05-12T16:59:02.444870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "match(iteration):\n",
    "    case 1:\n",
    "        selected_dataset = dataset_4\n",
    "    case 2:\n",
    "        selected_dataset = dataset_5\n",
    "    case 3: \n",
    "        selected_dataset = dataset_6\n",
    "    case 4: \n",
    "        selected_dataset = dataset_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56216beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:59:02.453607Z",
     "iopub.status.busy": "2024-05-12T16:59:02.453477Z",
     "iopub.status.idle": "2024-05-12T16:59:02.462807Z",
     "shell.execute_reply": "2024-05-12T16:59:02.462500Z"
    },
    "papermill": {
     "duration": 0.012268,
     "end_time": "2024-05-12T16:59:02.463896",
     "exception": false,
     "start_time": "2024-05-12T16:59:02.451628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX shape = (2211, 5, 5).\n",
      "TrainY shape = (2211, 1).\n",
      "TrainX shape = (23, 5, 5).\n",
      "TrainY shape = (23, 1).\n",
      "TrainX shape = (54, 5, 5).\n",
      "TrainY shape = (54, 1).\n"
     ]
    }
   ],
   "source": [
    "# Shaping Data\n",
    "def shape_data(data, n_future, n_past):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.to_numpy()\n",
    "    for i in range(n_past, data.shape[0] - n_future +1): \n",
    "        X.append(data[i - n_past:i, 0:data.shape[1]])\n",
    "        y.append(data[i + n_future - 1:i + n_future,data.shape[1]-1])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    print('TrainX shape = {}.'.format(X.shape))\n",
    "    print('TrainY shape = {}.'.format(y.shape))\n",
    "    return X, y\n",
    "\n",
    "n_future = config_['n_future']\n",
    "n_past = config_['n_past']\n",
    "trainX, trainY = shape_data(selected_dataset['train'], n_future, n_past)\n",
    "valX, valY = shape_data(selected_dataset['val'], n_future, n_past)\n",
    "testX, testY = shape_data(selected_dataset['test'], n_future, n_past)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f95b8043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T16:59:02.468138Z",
     "iopub.status.busy": "2024-05-12T16:59:02.467954Z",
     "iopub.status.idle": "2024-05-12T17:01:27.453035Z",
     "shell.execute_reply": "2024-05-12T17:01:27.452550Z"
    },
    "papermill": {
     "duration": 144.988709,
     "end_time": "2024-05-12T17:01:27.454405",
     "exception": false,
     "start_time": "2024-05-12T16:59:02.465696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 22:35:41.985027: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-06-07 22:35:41.985078: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-07 22:35:41.985091: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-07 22:35:41.985341: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-07 22:35:41.985672: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 5, 5)]            0         \n",
      "                                                                 \n",
      " encoder (Dense)             (None, 5, 100)            600       \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 500)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 100)            50100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 1)              101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50801 (198.44 KB)\n",
      "Trainable params: 50801 (198.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 22:35:43.009412: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 5s 36ms/step - loss: 0.0046 - val_loss: 0.0016\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 6.6152e-05 - val_loss: 9.0130e-04\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 4.2209e-05 - val_loss: 8.7165e-04\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 4.9097e-05 - val_loss: 7.5865e-04\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 3.9776e-05 - val_loss: 4.7165e-04\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 4.4894e-05 - val_loss: 2.3745e-04\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.0724e-05 - val_loss: 5.2563e-04\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 2.9558e-05 - val_loss: 5.8102e-04\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 6.1344e-05 - val_loss: 3.9235e-04\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 3.3514e-05 - val_loss: 2.6204e-04\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 3.3672e-05 - val_loss: 1.3925e-04\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.2905e-05 - val_loss: 4.2539e-05\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 3.5807e-05 - val_loss: 2.1376e-04\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 5.2613e-05 - val_loss: 1.4852e-04\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 3.2613e-05 - val_loss: 4.0305e-05\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 8.1118e-05 - val_loss: 4.3862e-05\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 6.1169e-05 - val_loss: 1.0458e-04\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 5.7636e-05 - val_loss: 3.9317e-05\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 3.3859e-05 - val_loss: 3.8003e-05\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 9.6540e-05 - val_loss: 9.9319e-04\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 6.1622e-05 - val_loss: 1.0716e-04\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.4005e-05 - val_loss: 1.9898e-04\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.7635e-05 - val_loss: 6.5303e-04\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.1881e-04 - val_loss: 2.1649e-04\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 9.3276e-05 - val_loss: 1.5590e-04\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.9778e-05 - val_loss: 6.9804e-05\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 3.6926e-05 - val_loss: 4.0214e-05\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 3.8026e-05 - val_loss: 2.1932e-04\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 5.3399e-05 - val_loss: 1.2342e-04\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.2563e-05 - val_loss: 4.4381e-05\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 5.6302e-05 - val_loss: 6.7121e-05\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 5.0270e-05 - val_loss: 9.8642e-05\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 7.8040e-05 - val_loss: 6.2121e-05\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 5.2721e-05 - val_loss: 4.7402e-04\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 7.0325e-05 - val_loss: 2.5519e-04\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 5.3301e-05 - val_loss: 4.1859e-04\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.9135e-05 - val_loss: 9.5136e-05\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 3.8171e-05 - val_loss: 6.2635e-04\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 3.3484e-05 - val_loss: 6.5697e-05\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 5, 5)]            0         \n",
      "                                                                 \n",
      " encoder (Dense)             (None, 5, 100)            600       \n",
      "                                                                 \n",
      " stochastic_dropout (Stocha  (None, 5, 100)            0         \n",
      " sticDropout)                                                    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 1, 500)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 1)              501       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1, 1)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1101 (4.30 KB)\n",
      "Trainable params: 501 (1.96 KB)\n",
      "Non-trainable params: 600 (2.34 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 4s 30ms/step - loss: 0.0422 - val_loss: 0.0436\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0041 - val_loss: 0.0048\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 9.3721e-04 - val_loss: 0.0013\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 6.4559e-04 - val_loss: 0.0011\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 4.6872e-04 - val_loss: 8.7979e-04\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.3803e-04 - val_loss: 8.5622e-04\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 2.6641e-04 - val_loss: 6.9853e-04\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 2.0756e-04 - val_loss: 6.1999e-04\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.7769e-04 - val_loss: 6.0691e-04\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.4498e-04 - val_loss: 6.0648e-04\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1930e-04 - val_loss: 5.9631e-04\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1516e-04 - val_loss: 5.3469e-04\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.0653e-04 - val_loss: 4.9018e-04\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.0056e-04 - val_loss: 5.0867e-04\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 9.0064e-05 - val_loss: 5.6487e-04\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 8.5922e-05 - val_loss: 4.4186e-04\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 8.8898e-05 - val_loss: 4.2552e-04\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 8.1246e-05 - val_loss: 4.4244e-04\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.3639e-05 - val_loss: 4.0353e-04\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.5765e-05 - val_loss: 4.2871e-04\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.0693e-05 - val_loss: 4.5468e-04\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 7.4333e-05 - val_loss: 4.0430e-04\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.7155e-05 - val_loss: 4.2291e-04\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.7772e-05 - val_loss: 4.4175e-04\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 6.8014e-05 - val_loss: 4.0045e-04\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 6.3148e-05 - val_loss: 3.9801e-04\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.4500e-05 - val_loss: 4.4497e-04\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 6.2345e-05 - val_loss: 3.3768e-04\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.0013e-05 - val_loss: 3.3185e-04\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 6.1552e-05 - val_loss: 3.4964e-04\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 5.7190e-05 - val_loss: 3.9979e-04\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 5.4371e-05 - val_loss: 3.5228e-04\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 5.4603e-05 - val_loss: 3.3651e-04\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 5.3579e-05 - val_loss: 2.6371e-04\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.2830e-05 - val_loss: 3.2659e-04\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 5.2102e-05 - val_loss: 3.1861e-04\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.9821e-05 - val_loss: 2.8639e-04\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.8399e-05 - val_loss: 2.6098e-04\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.7124e-05 - val_loss: 2.6803e-04\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.6012e-05 - val_loss: 2.4941e-04\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.4356e-05 - val_loss: 2.0827e-04\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.0239e-05 - val_loss: 2.6492e-04\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.4430e-05 - val_loss: 2.4936e-04\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.4986e-05 - val_loss: 2.8499e-04\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.2260e-05 - val_loss: 3.0955e-04\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.4240e-05 - val_loss: 2.1789e-04\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.0761e-05 - val_loss: 3.3226e-04\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.0846e-05 - val_loss: 2.1144e-04\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.8998e-05 - val_loss: 2.7066e-04\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.2104e-05 - val_loss: 2.3859e-04\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.0092e-05 - val_loss: 2.1937e-04\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.2625e-05 - val_loss: 1.7109e-04\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.2141e-05 - val_loss: 2.5688e-04\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.8467e-05 - val_loss: 2.5637e-04\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.0058e-05 - val_loss: 1.9266e-04\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.8084e-05 - val_loss: 2.0740e-04\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.7537e-05 - val_loss: 2.7467e-04\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.9783e-05 - val_loss: 1.8223e-04\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.9915e-05 - val_loss: 3.2251e-04\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.7942e-05 - val_loss: 2.0533e-04\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.7652e-05 - val_loss: 2.6475e-04\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.8654e-05 - val_loss: 1.6137e-04\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.7883e-05 - val_loss: 2.3622e-04\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.1098e-05 - val_loss: 3.0306e-04\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.6865e-05 - val_loss: 2.4852e-04\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.6754e-05 - val_loss: 2.7785e-04\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.6527e-05 - val_loss: 2.3324e-04\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.5651e-05 - val_loss: 1.8184e-04\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.8420e-05 - val_loss: 3.0102e-04\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.3960e-05 - val_loss: 1.9543e-04\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.6451e-05 - val_loss: 3.0939e-04\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.5175e-05 - val_loss: 1.9262e-04\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.7043e-05 - val_loss: 2.2420e-04\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.5764e-05 - val_loss: 2.1036e-04\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.6770e-05 - val_loss: 3.7033e-04\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.6860e-05 - val_loss: 2.5604e-04\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.5380e-05 - val_loss: 2.8700e-04\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.9585e-05 - val_loss: 2.3197e-04\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.1785e-05 - val_loss: 2.5505e-04\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.2204e-05 - val_loss: 4.0907e-04\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.5642e-05 - val_loss: 4.6829e-04\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.3238e-05 - val_loss: 5.3995e-04\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 5, 5)]            0         \n",
      "                                                                 \n",
      " encoder (Dense)             (None, 5, 100)            600       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5, 5)              505       \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 5, 5)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1105 (4.32 KB)\n",
      "Trainable params: 505 (1.97 KB)\n",
      "Non-trainable params: 600 (2.34 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 3s 26ms/step - loss: 0.1293 - val_loss: 0.0510\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0457 - val_loss: 0.0163\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0165 - val_loss: 0.0115\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0060 - val_loss: 0.0134\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0143\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0018 - val_loss: 0.0138\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0014 - val_loss: 0.0125\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0011 - val_loss: 0.0109\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 9.1605e-04 - val_loss: 0.0094\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 7.6629e-04 - val_loss: 0.0081\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.4532e-04 - val_loss: 0.0070\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 5.4674e-04 - val_loss: 0.0061\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.6453e-04 - val_loss: 0.0053\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.9719e-04 - val_loss: 0.0047\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.3947e-04 - val_loss: 0.0041\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.9092e-04 - val_loss: 0.0036\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 2.4930e-04 - val_loss: 0.0032\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.1455e-04 - val_loss: 0.0028\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8522e-04 - val_loss: 0.0025\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.6036e-04 - val_loss: 0.0023\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.3978e-04 - val_loss: 0.0021\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2176e-04 - val_loss: 0.0019\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.0667e-04 - val_loss: 0.0018\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 9.3976e-05 - val_loss: 0.0017\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 8.2979e-05 - val_loss: 0.0016\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.3684e-05 - val_loss: 0.0015\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 6.5805e-05 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 5.9038e-05 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.2894e-05 - val_loss: 0.0013\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 4.7768e-05 - val_loss: 0.0013\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.3201e-05 - val_loss: 0.0012\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.9384e-05 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.5704e-05 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.2610e-05 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 2.9917e-05 - val_loss: 0.0012\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.7461e-05 - val_loss: 0.0012\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.5369e-05 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.3499e-05 - val_loss: 0.0011\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.1988e-05 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.0392e-05 - val_loss: 0.0011\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.9168e-05 - val_loss: 0.0011\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8021e-05 - val_loss: 0.0011\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.6969e-05 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.6039e-05 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.5186e-05 - val_loss: 0.0011\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.4473e-05 - val_loss: 0.0011\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.3852e-05 - val_loss: 0.0011\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.3184e-05 - val_loss: 0.0011\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.2681e-05 - val_loss: 0.0011\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2137e-05 - val_loss: 0.0011\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1653e-05 - val_loss: 0.0011\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1273e-05 - val_loss: 0.0010\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.0875e-05 - val_loss: 0.0010\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.0464e-05 - val_loss: 0.0010\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.0174e-05 - val_loss: 0.0010\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 9.8650e-06 - val_loss: 0.0010\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 9.5270e-06 - val_loss: 9.9118e-04\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 9.2889e-06 - val_loss: 9.9259e-04\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 8.9563e-06 - val_loss: 9.9627e-04\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.7109e-06 - val_loss: 9.7997e-04\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.4940e-06 - val_loss: 9.7690e-04\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.2605e-06 - val_loss: 9.6672e-04\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 8.0159e-06 - val_loss: 9.5408e-04\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.8103e-06 - val_loss: 9.4854e-04\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.6514e-06 - val_loss: 9.2009e-04\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 7.4834e-06 - val_loss: 9.1909e-04\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.2600e-06 - val_loss: 9.0353e-04\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 7.0840e-06 - val_loss: 9.0818e-04\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.8775e-06 - val_loss: 8.9333e-04\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.7340e-06 - val_loss: 8.8278e-04\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.5446e-06 - val_loss: 8.7013e-04\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.3936e-06 - val_loss: 8.7384e-04\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 6.2536e-06 - val_loss: 8.4531e-04\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 6.0529e-06 - val_loss: 8.5191e-04\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.9470e-06 - val_loss: 8.4061e-04\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.8102e-06 - val_loss: 8.3054e-04\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.6514e-06 - val_loss: 8.0492e-04\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.4965e-06 - val_loss: 8.0985e-04\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 5.3434e-06 - val_loss: 7.8904e-04\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 5.1861e-06 - val_loss: 7.8444e-04\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 5.1066e-06 - val_loss: 7.7305e-04\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.9340e-06 - val_loss: 7.5382e-04\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.8156e-06 - val_loss: 7.4000e-04\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.7328e-06 - val_loss: 7.3915e-04\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.5791e-06 - val_loss: 7.2145e-04\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.4834e-06 - val_loss: 7.0351e-04\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.2990e-06 - val_loss: 7.0267e-04\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 4.2160e-06 - val_loss: 6.6930e-04\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 4.1069e-06 - val_loss: 6.6524e-04\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.9698e-06 - val_loss: 6.5637e-04\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.8491e-06 - val_loss: 6.5384e-04\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.7397e-06 - val_loss: 6.3308e-04\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.6199e-06 - val_loss: 6.1287e-04\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.5537e-06 - val_loss: 6.1118e-04\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 3.4515e-06 - val_loss: 5.9627e-04\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.3311e-06 - val_loss: 5.7406e-04\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.2232e-06 - val_loss: 5.7638e-04\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.1268e-06 - val_loss: 5.6401e-04\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 3.0142e-06 - val_loss: 5.5390e-04\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 2.9841e-06 - val_loss: 5.4632e-04\n"
     ]
    }
   ],
   "source": [
    "# how well the predicted uncertainty (in this case, represented by y_std) matches the actual accuracy of the predictions.\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "dropout_probability = 0.2\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "input_shape = config_['input_dim']\n",
    "encoding_dim = config_['encoding_dim']\n",
    "\n",
    "# predictor\n",
    "input_layer_1 = Input(shape=input_shape)\n",
    "encoder_output = Dense(encoding_dim, activation='relu', name=\"encoder\")(input_layer_1)\n",
    "flattened_output = Reshape((1,-1))(encoder_output)\n",
    "dense_layer = Dense(100)(flattened_output)\n",
    "predicted = Dense(1)(dense_layer)\n",
    "\n",
    "predictor_model = Model(inputs=input_layer_1, outputs=predicted)\n",
    "predictor_model.compile(optimizer='adam', loss='mse')\n",
    "print(predictor_model.summary())\n",
    "\n",
    "history_pm = predictor_model.fit(trainX, trainY, epochs=100, batch_size=32, validation_data=(valX, valY), verbose=1, callbacks=[early_stopping])\n",
    "encoder_layer = predictor_model.get_layer('encoder')\n",
    "\n",
    "# mc dropout predictor \n",
    "input_layer_2 = Input(shape=input_shape)\n",
    "encoded_data = encoder_layer(input_layer_2)\n",
    "encoder_layer.trainable = False\n",
    "dropped_data_1 = StochasticDropout(dropout_probability)(encoded_data)\n",
    "flattened_output = Reshape((1,-1))(dropped_data_1)  \n",
    "dense_output = Dense(1)(flattened_output)\n",
    "predicted_2 = LeakyReLU(alpha=0.3)(dense_output)\n",
    "\n",
    "mc_predictor_model = Model(inputs=input_layer_2, outputs=predicted_2)\n",
    "mc_predictor_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(mc_predictor_model.summary())\n",
    "\n",
    "history_mc_pm = mc_predictor_model.fit(trainX, trainY, epochs=100, batch_size=32, validation_data=(valX, valY), verbose=1, callbacks=[early_stopping])\n",
    "mc_model = StochasticRegressor(mc_predictor_model)\n",
    "\n",
    "#autoencoder\n",
    "input_layer = Input(shape=input_shape)\n",
    "encoded_data = encoder_layer(input_layer)\n",
    "encoder_layer.trainable = False\n",
    "decoder_output = Dense(5, activation='linear')(encoded_data)\n",
    "output_tensor = Reshape(input_shape)(decoder_output)\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_tensor)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "print(autoencoder.summary())\n",
    "\n",
    "history_ae = autoencoder.fit(trainX, trainX, epochs=100, batch_size=32, validation_data=(valX, valX), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bc6a556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T17:01:27.580942Z",
     "iopub.status.busy": "2024-05-12T17:01:27.580767Z",
     "iopub.status.idle": "2024-05-12T17:01:27.582773Z",
     "shell.execute_reply": "2024-05-12T17:01:27.582503Z"
    },
    "papermill": {
     "duration": 0.066131,
     "end_time": "2024-05-12T17:01:27.583636",
     "exception": false,
     "start_time": "2024-05-12T17:01:27.517505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = unscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0fd3999d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T17:01:27.716020Z",
     "iopub.status.busy": "2024-05-12T17:01:27.715857Z",
     "iopub.status.idle": "2024-05-12T18:43:46.057520Z",
     "shell.execute_reply": "2024-05-12T18:43:46.057063Z"
    },
    "papermill": {
     "duration": 6138.410797,
     "end_time": "2024-05-12T18:43:46.058928",
     "exception": false,
     "start_time": "2024-05-12T17:01:27.648131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/70 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 6ms/step\n",
      "70/70 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "2/2 [==============================] - 0s 299ms/step\n",
      "2/2 [==============================] - 0s 169ms/step\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "un_predictions = predictor_model.predict(trainX).flatten()\n",
    "predictions = scaler.inverse_transform(un_predictions.reshape(-1, 1))\n",
    "\n",
    "ae_output = autoencoder.predict(trainX)\n",
    "error_dict = {}\n",
    "for num, column in enumerate(data.columns):\n",
    "    error_dict[column+' error'] = []\n",
    "    for i in range(trainX.shape[0]):\n",
    "        error_dict[column+' error'].append(mean_absolute_percentage_error(ae_output[i,:,num], trainX[i,:,num]))\n",
    "    error_dict[column+' error'] = np.array(error_dict[column+' error'])\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i in range(trainX.shape[0]):\n",
    "    errors.append(mean_absolute_percentage_error(ae_output[i], trainX[i]))\n",
    "error_dict[\"errors\"] = np.array(errors)\n",
    "#shape is (iteration,day,column)\n",
    "\n",
    "gaussian_score = np.array([])\n",
    "calib_err = np.array([])\n",
    "for i in range(len(trainX)):\n",
    "    y_pred_mean, y_pred_std = mc_model.predict(trainX[i:i+1,:,:], num_samples=100)\n",
    "    gaussian_score = np.append(gaussian_score, y_pred_std)\n",
    "\n",
    "rl_train = pd.DataFrame(scaler.inverse_transform(selected_dataset['train'].iloc[5:].copy()), index=selected_dataset['train'].iloc[5:].index, columns=selected_dataset['train'].iloc[5:].columns)\n",
    "\n",
    "for error_type in error_dict.keys():\n",
    "    rl_train[error_type] = error_dict[error_type]\n",
    "\n",
    "rl_train['predictions'] = predictions\n",
    "rl_train['gauss_score'] = gaussian_score\n",
    "\n",
    "#val\n",
    "un_predictions = predictor_model.predict(valX).flatten()\n",
    "predictions = scaler.inverse_transform(un_predictions.reshape(-1, 1))\n",
    "\n",
    "ae_output = autoencoder.predict(valX)\n",
    "error_dict = {}\n",
    "for num, column in enumerate(data.columns):\n",
    "    error_dict[column+' error'] = []\n",
    "    for i in range(valX.shape[0]):\n",
    "        error_dict[column+' error'].append(mean_absolute_percentage_error(ae_output[i,:,num], valX[i,:,num]))\n",
    "    error_dict[column+' error'] = np.array(error_dict[column+' error'])\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i in range(valX.shape[0]):\n",
    "    errors.append(mean_absolute_percentage_error(ae_output[i], valX[i]))\n",
    "error_dict[\"errors\"] = np.array(errors)\n",
    "#shape is (iteration,day,column)\n",
    "\n",
    "gaussian_score = np.array([])\n",
    "calib_err = np.array([])\n",
    "for i in range(len(valX)):\n",
    "    y_pred_mean, y_pred_std = mc_model.predict(valX[i:i+1,:,:], num_samples=100)\n",
    "    gaussian_score = np.append(gaussian_score, y_pred_std)\n",
    "\n",
    "rl_val = pd.DataFrame(scaler.inverse_transform(selected_dataset['val'].iloc[5:].copy()), index=selected_dataset['val'].iloc[5:].index, columns=selected_dataset['val'].iloc[5:].columns)\n",
    "\n",
    "for error_type in error_dict.keys():\n",
    "    rl_val[error_type] = error_dict[error_type]\n",
    "\n",
    "rl_val['predictions'] = predictions\n",
    "rl_val['gauss_score'] = gaussian_score\n",
    "\n",
    "#test\n",
    "un_predictions = predictor_model.predict(testX).flatten()\n",
    "predictions = scaler.inverse_transform(un_predictions.reshape(-1, 1))\n",
    "\n",
    "ae_output = autoencoder.predict(testX)\n",
    "error_dict = {}\n",
    "for num, column in enumerate(data.columns):\n",
    "    error_dict[column+' error'] = []\n",
    "    for i in range(testX.shape[0]):\n",
    "        error_dict[column+' error'].append(mean_absolute_percentage_error(ae_output[i,:,num], testX[i,:,num]))\n",
    "    error_dict[column+' error'] = np.array(error_dict[column+' error'])\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i in range(testX.shape[0]):\n",
    "    errors.append(mean_absolute_percentage_error(ae_output[i], testX[i]))\n",
    "error_dict[\"errors\"] = np.array(errors)\n",
    "#shape is (iteration,day,column)\n",
    "\n",
    "gaussian_score = np.array([])\n",
    "calib_err = np.array([])\n",
    "for i in range(len(testX)):\n",
    "    y_pred_mean, y_pred_std = mc_model.predict(testX[i:i+1,:,:], num_samples=100)\n",
    "    gaussian_score = np.append(gaussian_score, y_pred_std)\n",
    "\n",
    "rl_test = pd.DataFrame(scaler.inverse_transform(selected_dataset['test'].iloc[5:].copy()), index=selected_dataset['test'].iloc[5:].index, columns=selected_dataset['test'].iloc[5:].columns)\n",
    "\n",
    "for error_type in error_dict.keys():\n",
    "    rl_test[error_type] = error_dict[error_type]\n",
    "\n",
    "rl_test['predictions'] = predictions\n",
    "rl_test['gauss_score'] = gaussian_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d31b5f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:43:46.189949Z",
     "iopub.status.busy": "2024-05-12T18:43:46.189657Z",
     "iopub.status.idle": "2024-05-12T18:43:46.268848Z",
     "shell.execute_reply": "2024-05-12T18:43:46.268461Z"
    },
    "papermill": {
     "duration": 0.146375,
     "end_time": "2024-05-12T18:43:46.269935",
     "exception": false,
     "start_time": "2024-05-12T18:43:46.123560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "\"\"\"Taken from finRL\"\"\"\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Provides methods for preprocessing the stock price data\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        use_technical_indicator : boolean\n",
    "            we technical indicator or not\n",
    "        tech_indicator_list : list\n",
    "            a list of technical indicator names (modified from neofinrl_config.py)\n",
    "        use_turbulence : boolean\n",
    "            use turbulence index or not\n",
    "        user_defined_feature:boolean\n",
    "            use user defined features or not\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    preprocess_data()\n",
    "        main method to do the feature engineering\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_technical_indicator=True,\n",
    "        tech_indicator_list = [\"macd\", \"boll\",\"boll_ub\",\"boll_lb\",\"rsi_30\",\"cci_30\",\"dx_30\",\"close_30_sma\",\"close_60_sma\"],\n",
    "        use_vix=False,\n",
    "        use_turbulence=False,\n",
    "        user_defined_feature=False,\n",
    "    ):\n",
    "        self.use_technical_indicator = use_technical_indicator\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.use_vix = use_vix\n",
    "        self.use_turbulence = use_turbulence\n",
    "        self.user_defined_feature = user_defined_feature\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"main method to do the feature engineering\n",
    "        @:param config: source dataframe\n",
    "        @:return: a DataMatrices object\n",
    "        \"\"\"\n",
    "        # clean data\n",
    "        df = self.clean_data(df)\n",
    "\n",
    "        # add technical indicators using stockstats\n",
    "        if self.use_technical_indicator:\n",
    "            df = self.add_technical_indicator(df)\n",
    "            print(\"Successfully added technical indicators\")\n",
    "\n",
    "        # add vix for multiple stock\n",
    "        if self.use_vix:\n",
    "            df = self.add_vix(df)\n",
    "            print(\"Successfully added vix\")\n",
    "\n",
    "        # add turbulence index for multiple stock\n",
    "        if self.use_turbulence:\n",
    "            df = self.add_turbulence(df)\n",
    "            print(\"Successfully added turbulence index\")\n",
    "\n",
    "        # add user defined feature\n",
    "        if self.user_defined_feature:\n",
    "            df = self.add_user_defined_feature(df)\n",
    "            print(\"Successfully added user defined features\")\n",
    "\n",
    "        # fill the missing values at the beginning and the end\n",
    "        df = df.ffill().bfill()\n",
    "        return df\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        \"\"\"\n",
    "        clean the raw data\n",
    "        deal with missing values\n",
    "        reasons: stocks could be delisted, not incorporated at the time step\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "        df.index = df.date.factorize()[0]\n",
    "        merged_closes = df.pivot_table(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        merged_closes = merged_closes.dropna(axis=1)\n",
    "        tics = merged_closes.columns\n",
    "        df = df[df.tic.isin(tics)]\n",
    "        # df = data.copy()\n",
    "        # list_ticker = df[\"tic\"].unique().tolist()\n",
    "        # only apply to daily level data, need to fix for minute level\n",
    "        # list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "        # combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "        # df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "        # df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "        # df_full = df_full.sort_values(['date','tic'])\n",
    "        # df_full = df_full.fillna(0)\n",
    "        return df\n",
    "\n",
    "    def add_technical_indicator(self, data):\n",
    "        \"\"\"\n",
    "        calculate technical indicators\n",
    "        use stockstats package to add technical inidactors\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values(by=[\"tic\", \"date\"])\n",
    "        stock = Sdf.retype(df.copy())\n",
    "        unique_ticker = stock.tic.unique()\n",
    "\n",
    "        for indicator in self.tech_indicator_list:\n",
    "            indicator_df = pd.DataFrame()\n",
    "            for i in range(len(unique_ticker)):\n",
    "                try:\n",
    "                    temp_indicator = stock[stock.tic == unique_ticker[i]][indicator]\n",
    "                    temp_indicator = pd.DataFrame(temp_indicator)\n",
    "                    temp_indicator[\"tic\"] = unique_ticker[i]\n",
    "                    temp_indicator[\"date\"] = df[df.tic == unique_ticker[i]][\n",
    "                        \"date\"\n",
    "                    ].to_list()\n",
    "                    # indicator_df = indicator_df.append(\n",
    "                    #     temp_indicator, ignore_index=True\n",
    "                    # )\n",
    "                    indicator_df = pd.concat(\n",
    "                        [indicator_df, temp_indicator], axis=0, ignore_index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            df = df.merge(\n",
    "                indicator_df[[\"tic\", \"date\", indicator]], on=[\"tic\", \"date\"], how=\"left\"\n",
    "            )\n",
    "        df = df.sort_values(by=[\"date\", \"tic\"])\n",
    "        return df\n",
    "        # df = data.set_index(['date','tic']).sort_index()\n",
    "        # df = df.join(df.groupby(level=0, group_keys=False).apply(lambda x, y: Sdf.retype(x)[y], y=self.tech_indicator_list))\n",
    "        # return df.reset_index()\n",
    "\n",
    "    def add_user_defined_feature(self, data):\n",
    "        \"\"\"\n",
    "         add user defined features\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df[\"daily_return\"] = df.close.pct_change(1)\n",
    "        # df['return_lag_1']=df.close.pct_change(2)\n",
    "        # df['return_lag_2']=df.close.pct_change(3)\n",
    "        # df['return_lag_3']=df.close.pct_change(4)\n",
    "        # df['return_lag_4']=df.close.pct_change(5)\n",
    "        return df\n",
    "\n",
    "    def add_vix(self, data):\n",
    "        \"\"\"\n",
    "        add vix from yahoo finance\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df_vix = YahooDownloader(\n",
    "            start_date=df.date.min(), end_date=df.date.max(), ticker_list=[\"^VIX\"]\n",
    "        ).fetch_data()\n",
    "        vix = df_vix[[\"date\", \"close\"]]\n",
    "        vix.columns = [\"date\", \"vix\"]\n",
    "\n",
    "        df = df.merge(vix, on=\"date\")\n",
    "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def add_turbulence(self, data):\n",
    "        \"\"\"\n",
    "        add turbulence index from a precalcualted dataframe\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        turbulence_index = self.calculate_turbulence(df)\n",
    "        df = df.merge(turbulence_index, on=\"date\")\n",
    "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def calculate_turbulence(self, data):\n",
    "        \"\"\"calculate turbulence index based on dow 30\"\"\"\n",
    "        # can add other market assets\n",
    "        df = data.copy()\n",
    "        df_price_pivot = df.pivot(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        # use returns to calculate turbulence\n",
    "        df_price_pivot = df_price_pivot.pct_change()\n",
    "\n",
    "        unique_date = df.date.unique()\n",
    "        # start after a year\n",
    "        start = 252\n",
    "        turbulence_index = [0] * start\n",
    "        # turbulence_index = [0]\n",
    "        count = 0\n",
    "        for i in range(start, len(unique_date)):\n",
    "            current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
    "            # use one year rolling window to calcualte covariance\n",
    "            hist_price = df_price_pivot[\n",
    "                (df_price_pivot.index < unique_date[i])\n",
    "                & (df_price_pivot.index >= unique_date[i - 252])\n",
    "            ]\n",
    "            # Drop tickers which has number missing values more than the \"oldest\" ticker\n",
    "            filtered_hist_price = hist_price.iloc[\n",
    "                hist_price.isna().sum().min() :\n",
    "            ].dropna(axis=1)\n",
    "\n",
    "            cov_temp = filtered_hist_price.cov()\n",
    "            current_temp = current_price[[x for x in filtered_hist_price]] - np.mean(\n",
    "                filtered_hist_price, axis=0\n",
    "            )\n",
    "            # cov_temp = hist_price.cov()\n",
    "            # current_temp=(current_price - np.mean(hist_price,axis=0))\n",
    "\n",
    "            temp = current_temp.values.dot(np.linalg.pinv(cov_temp)).dot(\n",
    "                current_temp.values.T\n",
    "            )\n",
    "            if temp > 0:\n",
    "                count += 1\n",
    "                if count > 2:\n",
    "                    turbulence_temp = temp[0][0]\n",
    "                else:\n",
    "                    # avoid large outlier because of the calculation just begins\n",
    "                    turbulence_temp = 0\n",
    "            else:\n",
    "                turbulence_temp = 0\n",
    "            turbulence_index.append(turbulence_temp)\n",
    "        try:\n",
    "            turbulence_index = pd.DataFrame(\n",
    "                {\"date\": df_price_pivot.index, \"turbulence\": turbulence_index}\n",
    "            )\n",
    "        except ValueError:\n",
    "            raise Exception(\"Turbulence information could not be added.\")\n",
    "        return turbulence_index\n",
    "\n",
    "\"\"\"Contains methods and classes to collect data from\n",
    "Yahoo Finance API\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "class YahooDownloader:\n",
    "    \"\"\"Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from neofinrl_config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from neofinrl_config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from neofinrl_config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start_date: str, end_date: str, ticker_list: list):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.ticker_list = ticker_list\n",
    "\n",
    "    def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
    "        \"\"\"Fetches data from Yahoo API\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        `pd.DataFrame`\n",
    "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
    "            for the specified stock ticker\n",
    "        \"\"\"\n",
    "        # Download and save the data in a pandas DataFrame:\n",
    "        data_df = pd.DataFrame()\n",
    "        num_failures = 0\n",
    "        for tic in self.ticker_list:\n",
    "            temp_df = yf.download(\n",
    "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
    "            )\n",
    "            temp_df[\"tic\"] = tic\n",
    "            if len(temp_df) > 0:\n",
    "                # data_df = data_df.append(temp_df)\n",
    "                data_df = pd.concat([data_df, temp_df], axis=0)\n",
    "            else:\n",
    "                num_failures += 1\n",
    "        if num_failures == len(self.ticker_list):\n",
    "            raise ValueError(\"no data is fetched.\")\n",
    "        # reset the index, we want to use numbers as index instead of dates\n",
    "        data_df = data_df.reset_index()\n",
    "        try:\n",
    "            # convert the column names to standardized names\n",
    "            data_df.columns = [\n",
    "                \"date\",\n",
    "                \"open\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                \"close\",\n",
    "                \"adjcp\",\n",
    "                \"volume\",\n",
    "                \"tic\",\n",
    "            ]\n",
    "            # use adjusted close price instead of close price\n",
    "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
    "            # drop the adjusted close price column\n",
    "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
    "        except NotImplementedError:\n",
    "            print(\"the features are not supported currently\")\n",
    "        # create day of the week column (monday = 0)\n",
    "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
    "        # convert date to standard string format, easy to filter\n",
    "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "        # drop missing data\n",
    "        data_df = data_df.dropna()\n",
    "        data_df = data_df.reset_index(drop=True)\n",
    "        print(\"Shape of DataFrame: \", data_df.shape)\n",
    "        # print(\"Display DataFrame: \", data_df.head())\n",
    "\n",
    "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
    "\n",
    "        return data_df\n",
    "\n",
    "    def select_equal_rows_stock(self, df):\n",
    "        df_check = df.tic.value_counts()\n",
    "        df_check = pd.DataFrame(df_check).reset_index()\n",
    "        df_check.columns = [\"tic\", \"counts\"]\n",
    "        mean_df = df_check.counts.mean()\n",
    "        equal_list = list(df.tic.value_counts() >= mean_df)\n",
    "        names = df.tic.value_counts().index\n",
    "        select_stocks_list = list(names[equal_list])\n",
    "        df = df[df.tic.isin(select_stocks_list)]\n",
    "        return df\n",
    "\n",
    "def add_indicators(og_df):\n",
    "    start_date = og_df.index[0]\n",
    "    end_date = og_df.index[-1]\n",
    "    downloader = YahooDownloader(start_date=start_date, end_date=end_date, ticker_list=['^SPX'])\n",
    "    new_data = downloader.fetch_data()\n",
    "    new_data['date'] = pd.to_datetime(new_data['date'])\n",
    "    df = pd.merge(left = new_data, right = og_df.reset_index().drop(columns = 'SnP PRICE').rename(columns={\"DATE\":\"date\"}), on='date')\n",
    "\n",
    "    fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = [\"macd\", \"macds\", \"macdh\", \"atr\",\"boll_ub\",\"boll_lb\",\"boll\", \"rsi\"],\n",
    "                     use_turbulence=False,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "    processed = fe.preprocess_data(df)\n",
    "    processed = processed.copy()\n",
    "    processed = processed.fillna(0)\n",
    "    processed = processed.replace(np.inf,0)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ab137fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T18:43:46.426746Z",
     "iopub.status.busy": "2024-05-12T18:43:46.426387Z",
     "iopub.status.idle": "2024-05-12T18:43:46.855930Z",
     "shell.execute_reply": "2024-05-12T18:43:46.855620Z"
    },
    "papermill": {
     "duration": 0.52366,
     "end_time": "2024-05-12T18:43:46.856884",
     "exception": false,
     "start_time": "2024-05-12T18:43:46.333224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (2133, 8)\n",
      "Successfully added technical indicators\n",
      "Shape of DataFrame:  (21, 8)\n",
      "Successfully added technical indicators\n",
      "Shape of DataFrame:  (52, 8)\n",
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_array = [\"rl_train\", \"rl_val\", \"rl_test\"]\n",
    "for i, items in enumerate([rl_train, rl_val, rl_test]):\n",
    "    items = add_indicators(items)\n",
    "    items.to_parquet(f'datasets/{temp_array[i]}_dataset{iteration+3}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34c540e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing:  50%|     | 7/14 [00:06<00:04,  1.50cell/s]2024-06-08 00:44:10.842785: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-06-08 00:44:10.842810: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-08 00:44:10.842820: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-08 00:44:10.842850: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 00:44:10.842871: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-08 00:44:11.787183: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "Executing: 100%|| 14/14 [1:07:30<00:00, 289.34s/cell]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing:  50%|     | 7/14 [00:06<00:04,  1.43cell/s]2024-06-08 01:51:41.935805: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-06-08 01:51:41.935827: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-08 01:51:41.935833: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-08 01:51:41.935873: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 01:51:41.935890: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-08 01:51:42.916171: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "Executing: 100%|| 14/14 [2:07:05<00:00, 544.67s/cell] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing:  50%|     | 7/14 [00:05<00:04,  1.52cell/s]2024-06-08 03:58:47.252969: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-06-08 03:58:47.252995: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-08 03:58:47.253003: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-08 03:58:47.253064: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 03:58:47.253085: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-08 03:58:48.235316: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "Executing: 100%|| 14/14 [1:27:32<00:00, 375.18s/cell]\n"
     ]
    }
   ],
   "source": [
    "import papermill as pm\n",
    "# parameters\n",
    "iterations = [1,2,3]\n",
    "\n",
    "for iteration in (iterations):\n",
    "    print(f\"iteration: {iteration}\")\n",
    "    print('========================')\n",
    "    pm.execute_notebook('dataset_preparation.ipynb',\n",
    "                        'dataset_preparation.ipynb',\n",
    "                        parameters=dict(iteration = iteration)\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ureca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6289.723958,
   "end_time": "2024-05-12T18:43:47.758415",
   "environment_variables": {},
   "exception": null,
   "input_path": "dataset_preparation.ipynb",
   "output_path": "dataset_preparation.ipynb",
   "parameters": {
    "iteration": 4
   },
   "start_time": "2024-05-12T16:58:58.034457",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
